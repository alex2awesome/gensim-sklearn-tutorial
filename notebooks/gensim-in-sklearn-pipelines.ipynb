{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this how-to tutorial, we demonstrate how Gensim libraries can be used in a 2-step supervised learning problem. \n",
    "\n",
    "* Step 0: Preprocessing.\n",
    "* Step 1: Text-embedding.\n",
    "* Step 2: Classification.\n",
    "\n",
    "As such, we will utilize external libraries like `sklearn` and `spacy` in some of these steps.\n",
    "\n",
    "We will compare the output of models using Gensim techniques with the output of models using other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>id</th>\n",
       "      <th>set</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: db7n+@andrew.cmu.edu (D. Andrew Byler)\\n...</td>\n",
       "      <td>21408</td>\n",
       "      <td>train</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mpaul@unl.edu (marxhausen paul)\\nSubject...</td>\n",
       "      <td>21388</td>\n",
       "      <td>train</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data     id    set  \\\n",
       "0  From: db7n+@andrew.cmu.edu (D. Andrew Byler)\\n...  21408  train   \n",
       "1  From: mpaul@unl.edu (marxhausen paul)\\nSubject...  21388  train   \n",
       "\n",
       "                    topic  \n",
       "0  soc.religion.christian  \n",
       "1  soc.religion.christian  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.load('20-newsgroups')\n",
    "news_df = pd.read_json(os.path.join(\"~\", \"gensim-data\", \"20-newsgroups\",  \"20-newsgroups.gz\"), lines=True)\n",
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: db7n+@andrew.cmu.edu (D. Andrew Byler)\n",
      "Subject: Re: Serbian genocide Work of God?\n",
      "Organization: Freshman, Civil Engineering, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 61\n",
      "\n",
      "Vera Shanti Noyes writes;\n",
      "\n",
      ">this is what indicates to me that you may believe in predestination.\n",
      ">am i correct?  i do not believe in predestination -- i believe we all\n",
      ">choose whether or not we will accept God's gift of salvation to us.\n",
      ">again, fundamental difference which can't really be resolved.\n",
      "\n",
      "Of course I believe in Predestination.  It's a very biblical doctrine as\n",
      "Romans 8.28-30 shows (among other passages).  Furthermore, the Church\n",
      "has always taught predestination, from the very beginning.  But to say\n",
      "that I believe in Predestination does not mean I do not believe in free\n",
      "will.  Men freely choose the course of their life, which is also\n",
      "affected by the grace of God.  However, unlike the Calvinists and\n",
      "Jansenists, I hold that grace is resistable, otherwise you end up with\n",
      "the idiocy of denying the universal s\n"
     ]
    }
   ],
   "source": [
    "print(news_df['data'][0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.sport.hockey            999\n",
       "soc.religion.christian      997\n",
       "rec.motorcycles             996\n",
       "rec.sport.baseball          994\n",
       "sci.crypt                   991\n",
       "rec.autos                   990\n",
       "sci.med                     990\n",
       "comp.windows.x              988\n",
       "sci.space                   987\n",
       "comp.os.ms-windows.misc     985\n",
       "sci.electronics             984\n",
       "comp.sys.ibm.pc.hardware    982\n",
       "misc.forsale                975\n",
       "comp.graphics               973\n",
       "comp.sys.mac.hardware       963\n",
       "talk.politics.mideast       940\n",
       "talk.politics.guns          910\n",
       "alt.atheism                 799\n",
       "talk.politics.misc          775\n",
       "talk.religion.misc          628\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start by defining our prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let's recast this multiclass dataset into a binary prediction problem by defining the following 2 classes:\n",
    "\n",
    "* `class 0 = {comp.*, sci.*}` articles\n",
    "* `class 1 = {talk.*, rec.*}` articles\n",
    "\n",
    "and building classifiers of the form $p(y | x)$ where $x$ is raw input text, and $y$ is one of the two class labels. Machine learning used to take the form of extensive data-analysis and feature-extraction. For example, we might start by analyzing the words being used in each `class`, and take counts of specific words, say \"machine\", or \"Iraq\". However, the beauty of the techniques offered in many modern machine-learning libraries, like Gensim, is that we can learn the relevant features based on our prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = (news_df\n",
    "           .loc[lambda df: df['topic'].str.contains('comp.|sci.|talk.|rec.')]\n",
    "           .assign(label=lambda df: df['topic'].apply(lambda x: 1 if ('comp' in x) or ('sci' in x) else 0 ))\n",
    "          )\n",
    "\n",
    "X_df = news_df['data'].reset_index(drop=True)\n",
    "y_df = news_df['label'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8843\n",
       "0    7232\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially with text-based processing, which often entails many steps, it's of great importance to be organized with training vs. test data. We will utilize `sklearn` Pipelines to properly separate our data. __A key error that is easy to make:__ performing an intermediate step on a full-dataset and then doing a test-train split for just the classification step. This results in signal leakage and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classifiers\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "## text transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "## pipeline and gridsearch\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "## tests\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from gensim.sklearn_api import D2VTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline__cv_lr = Pipeline([\n",
    "        ('cv', CountVectorizer(min_df=.05, max_df=.4)),\n",
    "        ('lr', LogisticRegressionCV(Cs=10, cv=10, max_iter=1000)),\n",
    "])\n",
    "\n",
    "pipeline__tfidf_lr = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(min_df=.05, max_df=.4)),\n",
    "        ('lr', LogisticRegressionCV(Cs=10, cv=10, max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to some pipelines using Gensim's `sklearn_api` package into the mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we have to develop some preprocessing steps. Gensim expects slightly different inputs for its classes than SKLearn. Here is a way to seamlessly integrate Gensim magic into your testing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import LdaTransformer\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.base import BaseEstimator, MetaEstimatorMixin\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(BaseEstimator, MetaEstimatorMixin):\n",
    "    \"\"\"Tokenize input strings based on a simple word-boundary pattern.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "        parser = lambda doc: token_pattern.findall(doc)\n",
    "        return X.apply(parser).values\n",
    "\n",
    "\n",
    "class Doc2BOW(BaseEstimator, MetaEstimatorMixin):\n",
    "    \"\"\"Transform a corpus into Bag-of-Word representation.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        self.dictionary = dictionary = Dictionary(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        output = []\n",
    "        for x in X:\n",
    "            output.append(dictionary.doc2bow(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are at it, we'll show how to extend another popular Gensim class, Word2Vec, to use this along with our other document-based pipelines. A common approach when using word-based embeddings is to represent the document as an average of such embeddings. We can easily extend Gensim's Word2Vec package to do just this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "import numpy as np\n",
    "\n",
    "class W2VTransformerDocLevel(W2VTransformer):\n",
    "    \"\"\"Extend Gensim's Word2Vec sklearn-wrapper class to further transform word-vectors into doc-vectors by\n",
    "    averaging the words in each document.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=1e-3, seed=1,\n",
    "                 workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
    "                 trim_rule=None, sorted_vocab=1, batch_words=10000):\n",
    "        super().__init__(size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words)\n",
    "    \n",
    "    def transform(self, docs):      \n",
    "        doc_vecs = []\n",
    "        for doc in docs:\n",
    "            ## for each document generate a word matrix\n",
    "            word_vectors_per_doc = []\n",
    "            for word in doc:\n",
    "                ## handle out-of vocabulary words\n",
    "                if word in self.gensim_model.wv:\n",
    "                    word_vectors_per_doc.append(self.gensim_model.wv[word])\n",
    "                    \n",
    "            word_vectors_per_doc = np.array(word_vectors_per_doc)\n",
    "            ## take the column-wise mean of this matrix and store\n",
    "            doc_vecs.append(word_vectors_per_doc.mean(axis=0))\n",
    "        return np.array(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline__lda = Pipeline([\n",
    "    ('tokenize', MyTokenizer()),\n",
    "    ('doc2bow', Doc2BOW()),\n",
    "    ('lda', LdaTransformer(num_topics=200, iterations=50)),\n",
    "    ('lr', LogisticRegressionCV(Cs=10, cv=10, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline__d2v = Pipeline([\n",
    "    ('tokenize', MyTokenizer()),\n",
    "    ('d2v', D2VTransformer(size=200, iter=50)),\n",
    "    ('lr', LogisticRegressionCV(Cs=10, cv=10, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline__w2v = Pipeline([\n",
    "    ('tokenize', MyTokenizer()),\n",
    "    ('w2v', W2VTransformerDocLevel(size=200, iter=50)),\n",
    "    ('lr', LogisticRegressionCV(Cs=10, cv=10, max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'cv_lr': pipeline__cv_lr,\n",
    "    'tfidf_lr': pipeline__tfidf_lr,\n",
    "    'lda_lr': pipeline__lda,\n",
    "    'w2v_lr': pipeline__w2v,\n",
    "    'd2v_lr': pipeline__d2v\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold 0 TRAIN: [ 5359  5360  5361 ... 16072 16073 16074] TEST: [   0    1    2 ... 5356 5357 5358]\n",
      "fitting cv_lr...\n",
      "fitting tfidf_lr...\n",
      "fitting lda_lr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1030: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting w2v_lr...\n",
      "fitting d2v_lr...\n",
      "kfold 1 TRAIN: [    0     1     2 ... 16072 16073 16074] TEST: [ 5359  5360  5361 ... 10714 10715 10716]\n",
      "fitting cv_lr...\n",
      "fitting tfidf_lr...\n",
      "fitting lda_lr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/ldamodel.py:1030: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting w2v_lr...\n",
      "fitting d2v_lr...\n"
     ]
    }
   ],
   "source": [
    "scores = defaultdict(dict)\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "for k_fold, chunk in enumerate(kf.split(X_df)):\n",
    "    train_index, test_index = chunk\n",
    "    print(\"kfold\", k_fold, \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_df.loc[train_index], X_df.loc[test_index]\n",
    "    y_train, y_test = y_df.loc[train_index], y_df.loc[test_index]\n",
    "    \n",
    "    for pipe_name, pipe in pipelines.items():\n",
    "        print(\"fitting %s...\" % pipe_name)\n",
    "        \n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict_proba(X_test)\n",
    "        scores[pipe_name][k_fold] = roc_auc_score(y_test, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "* Dictionary2Bow -- can't that be fit into the LDA Transformer?\n",
    "* LDA, D2V, W2v check for `doc[0]`..... this fails when using non zero-indexed inputs like dataframes.\n",
    "* W2VTransformer doesn't have any handling for train/test splits and fails on out-of-vocabulary errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Additional preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dirty magic of text-data processing success often lies in how well you preprocess your data. While there are many things that can be done, they are often task-dependent. Here, we give some methods for several light preprocessing:\n",
    "\n",
    "* __Lemmatize__: take the root form of a word (eg. \"running\"=>\"run\")\n",
    "* __Remove stopwords__: commonly-used words that carry little meaning (eg. \"and\", \"the\")\n",
    "* __Identify part-of-speech tags__: determine if a word is a \"Verb\" or a \"Noun\". This can help for commonly used words like \"well\", that have different meanings based on whether they appear as a noun or, say, and adjective.\n",
    "* __Identify noun-phrases__: determine noun-chunks and join these together. This can help for compound phrases that carry meaning together but are meaningless seperately (ex. \"climate change\" carries different meaning than \"climate\" or \"change\" separately).\n",
    "\n",
    "but note that there's a lot more that can be done. Care should be taken to explore this part of your classification task: often, the \"black magic\" of a good classifier vs. an underperforming one lies in how well you preprocess.\n",
    "\n",
    "\\** Note that often one can preprocessed data outside of a pipeline. A good rule-of-thumb in determining whether to chain operations or not is whether that step learns internal parameters. No matter what they are, say, a threshold like in CountVectorizer, or an embedding matrix as in Doc2Vec, it's a good idea to train on training data separately. Since our preprocessing is stateless -- i.e., we did not learn a model -- and time-consuming, we can safely process the entire set beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time\n",
    "from spacy.parts_of_speech import *\n",
    "from nltk.corpus import stopwords\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## X \n",
    "def preprocess_text_line(text):\n",
    "    \"\"\"Simple preprocessing to:\n",
    "        * lemmatize\n",
    "        * remove stopwords and punctuation\n",
    "        * concatenate POS to word for disambiguation.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        if (word.pos not in (PUNCT, SPACE)) and (word.lemma_ not in stopwords):\n",
    "            processed = word.lemma_ + \"-\" + word.pos_\n",
    "            output.append(processed)\n",
    "        \n",
    "    return ' '.join(output)\n",
    "\n",
    "def preprocess_noun_phrase_line(text):\n",
    "    \"\"\"Nounphrase preprocessing that:\n",
    "        * extracts noun phrases\n",
    "        * lemmatizes\n",
    "        * removes stopwords and punctuation\n",
    "    \"\"\"\n",
    "    output = []\n",
    "      \n",
    "    doc = nlp(text)\n",
    "    for noun_phrase in doc.noun_chunks:\n",
    "        noun_phrase_output = []\n",
    "        for word in noun_phrase:\n",
    "            if (word.pos not in (PUNCT, SPACE, NUM)) and (word.lemma_ not in stopwords):\n",
    "                noun_phrase_output.append(word.lemma_)\n",
    "        noun_phrase_output = '-'.join(noun_phrase_output)\n",
    "        output.append(noun_phrase_output)\n",
    "        \n",
    "    return ' '.join(output)\n",
    "\n",
    "\n",
    "def time_process_X(input_X, processing_func):\n",
    "    \"\"\"Run process_text_line over a DF and time the output. Return series of processed data.\"\"\"\n",
    "    processed_X = []\n",
    "    now = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for idx, x, in enumerate(executor.map(processing_func, input_X)): ##\n",
    "            if idx % 2000 == 0:\n",
    "                print(\"finished %d in %f...\" % (idx, time.time()- now))\n",
    "                now= time.time()\n",
    "\n",
    "            processed_X.append(x) ##\n",
    "        \n",
    "    return pd.Series(processed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0 in 2.782411...\n",
      "finished 2000 in 88.621636...\n",
      "finished 4000 in 169.495885...\n",
      "finished 6000 in 134.525473...\n",
      "finished 8000 in 146.415887...\n",
      "finished 10000 in 140.682751...\n",
      "finished 12000 in 87.144323...\n",
      "finished 14000 in 100.570627...\n",
      "finished 16000 in 112.727224...\n"
     ]
    }
   ],
   "source": [
    "X_df = time_process_X(X_df, preprocess_text_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ds@aris.nswc.navy.mil-PROPN demetrios-PROPN sapounas-PROPN subject-NOUN 3d-ADJ display-NOUN software-NOUN organization-PROPN nswc-PROPN lines-PROPN 19-NUM -PRON--PRON need-NOUN display-VERB 2-NUM 1/2-NUM surface-NOUN x-NOUN use-VERB xlib-PROPN xt-PROPN xm-PROPN anyone-NOUN know-VERB package-NOUN available-ADJ internet-NOUN able-ADJ work-NOUN -PRON--PRON look-VERB stand-VERB alone-ADJ package-NOUN provide-VERB similar-ADJ function-NOUN xprism3-PROPN available-ADJ khoros-PROPN without-ADP numerous-ADJ library-NOUN require-VERB -PRON--PRON -PRON--PRON want-VERB able-ADJ recompile-VERB -PRON--PRON run-VERB -PRON--PRON various-ADJ platform-NOUN sgi-NOUN i486s-PROPN unix-PROPN help-NOUN appreciate-VERB =-X =-SYM demetrios-PROPN sapounas-PROPN tel-PROPN +-SYM 1-NUM 703-NUM 663.8332-NUM l-PROPN 115-NUM nswc-PROPN fax-NOUN +-SYM 1-NUM 703-NUM 663.1939-NUM dahlgren-PROPN va-PROPN 22448-NUM --SYM 5000-NUM usa-PROPN email-NOUN ds@aris.nswc.navy.mil-PROPN =-SYM =-SYM =-SYM'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
